{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in CVR_C, using vit_small_patch\n",
    "\n",
    "Shape example (for 224x224, patch_size=16):\n",
    "Image x: (B,3,224,224)\n",
    "After patch_embed: (B, 196, D) - 14x14 = 196 patches\n",
    "\n",
    "Starting with seeding for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard imports\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "#PyTorch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "#torchvision\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "#logging (Lightning handles TensorBoard automatically)\n",
    "\n",
    "#set random seeds for reproducibility\n",
    "torch.manual_seed(31)\n",
    "np.random.seed(31)\n",
    "random.seed(31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data transforms.  Define augmentation for training and resizing/cropping for validation.  Lightning will use these transforms in the DataModule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates masks for custom attention module (5 spatial relations + identity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRowsAndCols(image, patch_size):\n",
    "    \"\"\"Compute center row/column of each patch.\"\"\"\n",
    "    _, H, W = image.shape\n",
    "    num_patches_y = H // patch_size\n",
    "    num_patches_x = W // patch_size\n",
    "    token_rows, token_cols = [], []\n",
    "    for i in range(num_patches_y):\n",
    "        for j in range(num_patches_x):\n",
    "            token_rows.append(i * patch_size + patch_size / 2)\n",
    "            token_cols.append(j * patch_size + patch_size / 2)\n",
    "    return np.array(token_rows), np.array(token_cols)\n",
    "\n",
    "def makeMasks(token_rows, token_cols):\n",
    "    \"\"\"Generate left, right, up, down, identity masks.\"\"\"\n",
    "    num_patches = len(token_rows)\n",
    "    left_mask = np.zeros((num_patches, num_patches))\n",
    "    right_mask = np.zeros((num_patches, num_patches))\n",
    "    up_mask = np.zeros((num_patches, num_patches))\n",
    "    down_mask = np.zeros((num_patches, num_patches))\n",
    "\n",
    "    for i in range(num_patches):\n",
    "        for j in range(num_patches):\n",
    "            if token_cols[i] < token_cols[j]: left_mask[i, j] = 1\n",
    "            if token_cols[i] > token_cols[j]: right_mask[i, j] = 1\n",
    "            if token_rows[i] < token_rows[j]: up_mask[i, j] = 1\n",
    "            if token_rows[i] > token_rows[j]: down_mask[i, j] = 1\n",
    "\n",
    "    identity_mask = np.eye(num_patches)\n",
    "    return left_mask, right_mask, up_mask, down_mask, identity_mask\n",
    "\n",
    "def create_mask_list_for_image(image, patch_size, num_heads, device, dtype):\n",
    "    \"\"\"Compute 5 spatial masks and expand for attention heads and CLS token.\"\"\"\n",
    "    token_rows, token_cols = getRowsAndCols(image, patch_size)\n",
    "    left_np, right_np, up_np, down_np, identity_np = makeMasks(token_rows, token_cols)\n",
    "\n",
    "    # Pad for CLS token\n",
    "    left_np = np.pad(left_np, ((1,0),(1,0)), constant_values=1)\n",
    "    right_np = np.pad(right_np, ((1,0),(1,0)), constant_values=1)\n",
    "    up_np = np.pad(up_np, ((1,0),(1,0)), constant_values=1)\n",
    "    down_np = np.pad(down_np, ((1,0),(1,0)), constant_values=1)\n",
    "    identity_np = np.pad(identity_np, ((1,0),(1,0)), constant_values=1)\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    def to_tensor(np_mask):\n",
    "        return torch.tensor(np_mask, device=device, dtype=dtype).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    left_mask = to_tensor(left_np).expand(1, num_heads, left_np.shape[0], left_np.shape[1])\n",
    "    right_mask = to_tensor(right_np).expand(1, num_heads, right_np.shape[0], right_np.shape[1])\n",
    "    up_mask = to_tensor(up_np).expand(1, num_heads, up_np.shape[0], up_np.shape[1])\n",
    "    down_mask = to_tensor(down_np).expand(1, num_heads, down_np.shape[0], down_np.shape[1])\n",
    "    identity_mask = to_tensor(identity_np).expand(1, num_heads, identity_np.shape[0], identity_np.shape[1])\n",
    "\n",
    "    return (left_mask, right_mask, up_mask, down_mask, identity_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset generates attention masks on the fly for each image.  This mirrors custom attention setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnTheFlyMaskedFood101(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, split, transform, patch_size, num_heads, download=True):\n",
    "        self.dataset = torchvision.datasets.Food101(\n",
    "            root=root,\n",
    "            split=split,\n",
    "            transform=transform,\n",
    "            download=download\n",
    "        )\n",
    "        self.patch_size = patch_size\n",
    "        self.num_heads = num_heads\n",
    "        self.dtype = torch.float32\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        ''' creates datasets in cpu and hands them off\n",
    "            it avoids keeping large datasets in GPU memory\n",
    "            (very wasteful).  Move only the batch needed to GPU during training. '''\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        mask_list = create_mask_list_for_image(\n",
    "            image,\n",
    "            self.patch_size,\n",
    "            self.num_heads,\n",
    "            device=self.device,\n",
    "            dtype=self.dtype)\n",
    "        return image, label, mask_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implements five-spatial-masks attention for ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAttentionMultipleFiveSpatial(nn.Module):\n",
    "    def __init__(self, orig_attn: nn.Module, patch_size=16, img_size=224):\n",
    "        super().__init__()\n",
    "        self.num_heads = orig_attn.num_heads\n",
    "        self.embed_dim = orig_attn.qkv.in_features\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
    "        self.patch_size = patch_size\n",
    "        self.img_size = img_size\n",
    "\n",
    "        # Separate Q, V, and 5 K linear layers\n",
    "        self.q_linear = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.v_linear = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.kA_linear = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.kB_linear = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.kC_linear = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.kD_linear = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.kE_linear = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.proj = orig_attn.proj\n",
    "\n",
    "        # Initialize weights from original attention module\n",
    "        qkv_weight = orig_attn.qkv.weight.clone()\n",
    "        qkv_bias = orig_attn.qkv.bias.clone() if orig_attn.qkv.bias is not None else None\n",
    "        self.q_linear.weight.data.copy_(qkv_weight[:self.embed_dim, :])\n",
    "        self.kA_linear.weight.data.copy_(qkv_weight[self.embed_dim:2*self.embed_dim, :])\n",
    "        self.kB_linear.weight.data.copy_(qkv_weight[self.embed_dim:2*self.embed_dim, :])\n",
    "        self.kC_linear.weight.data.copy_(qkv_weight[self.embed_dim:2*self.embed_dim, :])\n",
    "        self.kD_linear.weight.data.copy_(qkv_weight[self.embed_dim:2*self.embed_dim, :])\n",
    "        self.kE_linear.weight.data.copy_(qkv_weight[self.embed_dim:2*self.embed_dim, :])\n",
    "        self.v_linear.weight.data.copy_(qkv_weight[2*self.embed_dim:, :])\n",
    "\n",
    "        if qkv_bias is not None:\n",
    "            self.q_linear.bias.data.copy_(qkv_bias[:self.embed_dim])\n",
    "            self.kA_linear.bias.data.copy_(qkv_bias[self.embed_dim:2*self.embed_dim])\n",
    "            self.kB_linear.bias.data.copy_(qkv_bias[self.embed_dim:2*self.embed_dim])\n",
    "            self.kC_linear.bias.data.copy_(qkv_bias[self.embed_dim:2*self.embed_dim])\n",
    "            self.kD_linear.bias.data.copy_(qkv_bias[self.embed_dim:2*self.embed_dim])\n",
    "            self.kE_linear.bias.data.copy_(qkv_bias[self.embed_dim:2*self.embed_dim])\n",
    "            self.v_linear.bias.data.copy_(qkv_bias[2*self.embed_dim:])\n",
    "\n",
    "    def forward(self, query, key=None, value=None, key_padding_mask=None, need_weights=True, attn_mask=None):\n",
    "        if key is None: key = query\n",
    "        if value is None: value = query\n",
    "        if not hasattr(self, 'current_mask_list'):\n",
    "            raise ValueError(\"current_mask_list not set!\")\n",
    "\n",
    "        left_mask, right_mask, up_mask, down_mask, identity_mask = self.current_mask_list\n",
    "        B, N, _ = query.shape\n",
    "\n",
    "        # Linear projections and reshape\n",
    "        q = self.q_linear(query).reshape(B, N, self.num_heads, self.head_dim).transpose(1, 2) * self.scale\n",
    "        v = self.v_linear(query).reshape(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k_list = [self.kA_linear, self.kB_linear, self.kC_linear, self.kD_linear, self.kE_linear]\n",
    "        attn_masks = [left_mask, right_mask, up_mask, down_mask, identity_mask]\n",
    "\n",
    "        # Compute masked attention for each K\n",
    "        out = 0\n",
    "        for k_linear, mask in zip(k_list, attn_masks):\n",
    "            k = k_linear(query).reshape(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "            attn = (q @ k.transpose(-2, -1)).softmax(dim=-1) * mask\n",
    "            out += attn @ v\n",
    "\n",
    "        out = out.transpose(1, 2).reshape(B, N, self.embed_dim)\n",
    "        return self.proj(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates ViT with custom attention and fewer blocks to reduce computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.vision_transformer import vit_small_patch16_224\n",
    "\n",
    "class ViTCustom(nn.Module):\n",
    "    def __init__(self, num_blocks_to_keep, patch_size=16, img_size=224):\n",
    "        super().__init__()\n",
    "        full_model = vit_small_patch16_224(pretrained=False, num_classes=101, drop_rate=0.3, drop_path_rate=0.1)\n",
    "\n",
    "        self.patch_embed = full_model.patch_embed\n",
    "        self.cls_token = full_model.cls_token\n",
    "        self.pos_embed = full_model.pos_embed\n",
    "        self.pos_drop = full_model.pos_drop\n",
    "\n",
    "        self.blocks = nn.Sequential()\n",
    "        for i, block in enumerate(full_model.blocks[:num_blocks_to_keep]):\n",
    "            if hasattr(block, 'attn'):\n",
    "                block.attn = CustomAttentionMultipleFiveSpatial(block.attn, patch_size, img_size)\n",
    "            self.blocks.append(block)\n",
    "\n",
    "        self.norm = full_model.norm\n",
    "        self.head = full_model.head\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        return self.head(x[:, 0])\n",
    "\n",
    "# # Instantiate model\n",
    "# patch_size = 16\n",
    "# num_blocks_to_keep = 10\n",
    "# model = ViTCustom(num_blocks_to_keep=num_blocks_to_keep, patch_size=patch_size)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataModule\n",
    "\n",
    "    Lightning DataModules separate data preparation and data loaders.\n",
    "\n",
    "    *setup initializes datasets\n",
    "    train_dataloader and val_dataloader returns DataLoaders with persistent workers for speed.\n",
    "\n",
    "    Also, with jupyter notebook with num_workers > 0 code could hang without persistent workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Food101DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, \n",
    "                 data_dir='./data', \n",
    "                 batch_size=32,\n",
    "                 patch_size=16,\n",
    "                 num_heads=10, \n",
    "                 num_workers=2):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_heads = num_heads\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = OnTheFlyMaskedFood101(\n",
    "            root=self.data_dir,\n",
    "            split='train',\n",
    "            transform=train_transform,\n",
    "            patch_size=self.patch_size,\n",
    "            num_heads=self.num_heads,\n",
    "            download=True\n",
    "        )\n",
    "        self.val_dataset = OnTheFlyMaskedFood101(\n",
    "            root=self.data_dir,\n",
    "            split='test',\n",
    "            transform=val_transform,\n",
    "            patch_size=self.patch_size,\n",
    "            num_heads=self.num_heads,\n",
    "            download=True\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=True\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, training_step and validation_step replace manual loops.\n",
    "self.log() automatically logs to TensorBoard.\n",
    "Optimizer + learning rate schedule are defined in configure_optimizers.\n",
    "Custom attention masks are applied on the fly just like PyTorch Loop from initial code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitViT(pl.LightningModule):\n",
    "    def __init__(self, num_blocks_to_keep=10, patch_size=16, img_size=224,\n",
    "                 num_classes=101, base_lr=1e-5, peak_lr=1e-4, final_lr_frac=0.1,\n",
    "                 warmup_epochs=15, rampup_epochs=15, num_epochs=60, weight_decay=0.5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = ViTCustom(num_blocks_to_keep=num_blocks_to_keep,\n",
    "                               patch_size=patch_size, img_size=img_size)\n",
    "        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        self.epoch_start_time = None\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        self.epoch_start_time = time.time()\n",
    "        print(f\"\\nStarting epoch {self.current_epoch + 1}/{self.hparams.num_epochs}\")\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        duration = time.time() - self.epoch_start_time\n",
    "        print(f\"Epoch {self.current_epoch + 1} completed in {duration:.2f} seconds\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, mask_list = batch\n",
    "        batch_mask_list = tuple(m.squeeze(1) for m in mask_list)\n",
    "        for blk in self.model.blocks:\n",
    "            blk.attn.current_mask_list = batch_mask_list\n",
    "        out = self(x)\n",
    "        loss = self.criterion(out, y)\n",
    "        acc = (out.argmax(dim=1) == y).float().mean()\n",
    "        self.log('train_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', acc, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, mask_list = batch\n",
    "        batch_mask_list = tuple(m.squeeze(1) for m in mask_list)\n",
    "        for blk in self.model.blocks:\n",
    "            blk.attn.current_mask_list = batch_mask_list\n",
    "        out = self(x)\n",
    "        loss = self.criterion(out, y)\n",
    "        acc = (out.argmax(dim=1) == y).float().mean()\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', acc, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.base_lr, weight_decay=self.hparams.weight_decay)\n",
    "\n",
    "        def lr_lambda(step):\n",
    "            steps_per_epoch = self.trainer.estimated_stepping_batches / self.hparams.num_epochs\n",
    "            warmup_steps = steps_per_epoch * self.hparams.warmup_epochs\n",
    "            rampup_steps = steps_per_epoch * self.hparams.rampup_epochs\n",
    "            decay_steps = self.trainer.estimated_stepping_batches - warmup_steps - rampup_steps\n",
    "\n",
    "            if step < warmup_steps:\n",
    "                return 1.0\n",
    "            elif step < warmup_steps + rampup_steps:\n",
    "                progress = (step - warmup_steps) / rampup_steps\n",
    "                scaled_lr = self.hparams.base_lr + progress * (self.hparams.peak_lr - self.hparams.base_lr)\n",
    "                return scaled_lr / self.hparams.base_lr\n",
    "            else:\n",
    "                progress = (step - warmup_steps - rampup_steps) / max(1, decay_steps)\n",
    "                cosine = 0.5 * (1 + math.cos(math.pi * progress))\n",
    "                scaled_lr = self.hparams.final_lr_frac * self.hparams.peak_lr + (1 - self.hparams.final_lr_frac) * self.hparams.peak_lr * cosine\n",
    "                return scaled_lr / self.hparams.base_lr\n",
    "\n",
    "        scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "        return [optimizer], [scheduler]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lightning automatically handles epoch/step loops, device placement, logging, checkpointing.\n",
    "TensorBoard logs will appear under tb_logs/food101_vit\n",
    "Model checkpoints are saved automatically based on validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# Initialize DataModule\n",
    "data_module = Food101DataModule(batch_size=32, patch_size=16, num_heads=10)\n",
    "\n",
    "# Initialize LightningModule\n",
    "lit_model = LitViT(num_blocks_to_keep=10, patch_size=16, img_size=224,\n",
    "                   num_epochs=60, base_lr=1e-5, peak_lr=1e-4, final_lr_frac=0.1)\n",
    "\n",
    "# Set up checkpoint callback to save the best model based on validation accuracy\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_acc\",        # the metric to monitor\n",
    "    mode=\"max\",               # save the model with max val_acc\n",
    "    save_top_k=1,             # save only the best model\n",
    "    filename=\"best-vit-{epoch:02d}-{val_acc:.4f}\"  # optional formatting\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=60,\n",
    "    accelerator='gpu',  # or 'cpu' if no GPU\n",
    "    devices=1,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    log_every_n_steps=10\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.fit(lit_model, datamodule=data_module)\n",
    "\n",
    "# Path to best checkpoint\n",
    "print(f\"Best model saved at: {checkpoint_callback.best_model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
