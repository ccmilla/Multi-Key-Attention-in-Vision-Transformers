{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxDuGUw2p-By"
      },
      "source": [
        "# Training vit_small_patch16_224 from scratch (no pre-trained weights) on the Food-101 dataset with the multi-key attention block #\n",
        "## Author - Thomas O'Sullivan ##\n",
        "\n",
        "### This notebook reduces the model to have 10/12 attention blocks, a drop rate of 0.3, batch size of 32, weight decay of 1e-4, and is trained with a base learning rate of 1e-5 as it progresses through our LR schedule defined in cell 7. ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s53mdOB6GSdm"
      },
      "source": [
        "### This cell imports libraries for deep learning, data handling, and visualization, and sets random seeds for reproducibility. ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pCoIT-xOhOG_"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import copy\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "torch.manual_seed(31)\n",
        "random.seed(31)\n",
        "np.random.seed(31)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5TzqJv2GWGj"
      },
      "source": [
        "### This cell defines image transformations for training and validation. Training data is augmented with cropping, flipping, and color jittering, while validation data is resized and center cropped. Both are converted to tensors and normalized using ImageNet statistics. ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GgnYr9z0p3LB"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvYT-dx8HRfp"
      },
      "source": [
        "### This cell defines functions to generate spatial attention masks for Vision Transformer patches. It calculates patch center positions, builds directional masks (left, right, up, down, identity), and pads them to include the CLS token. The final output is a tuple of 5 expanded torch masks shaped for multi-head attention. ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5gEMFn8Uho-v"
      },
      "outputs": [],
      "source": [
        "def getRowsAndCols(image, patch_size):\n",
        "    \"\"\"\n",
        "    Given an image tensor of shape (C, H, W) and a patch size,\n",
        "    compute the center row and column of each patch.\n",
        "    \"\"\"\n",
        "    _, H, W = image.shape\n",
        "    num_patches_y = H // patch_size\n",
        "    num_patches_x = W // patch_size\n",
        "    token_rows = []\n",
        "    token_cols = []\n",
        "    for i in range(num_patches_y):\n",
        "        for j in range(num_patches_x):\n",
        "            center_row = i * patch_size + patch_size / 2\n",
        "            center_col = j * patch_size + patch_size / 2\n",
        "            token_rows.append(center_row)\n",
        "            token_cols.append(center_col)\n",
        "    return np.array(token_rows), np.array(token_cols)\n",
        "\n",
        "def makeMasks(token_rows, token_cols):\n",
        "    \"\"\"\n",
        "    Generate 5 spatial masks:\n",
        "      left_mask, right_mask, up_mask, down_mask, identity_mask\n",
        "    Each is (num_patches, num_patches).\n",
        "    \"\"\"\n",
        "    num_patches = len(token_rows)\n",
        "    left_mask = np.zeros((num_patches, num_patches))\n",
        "    right_mask = np.zeros((num_patches, num_patches))\n",
        "    up_mask = np.zeros((num_patches, num_patches))\n",
        "    down_mask = np.zeros((num_patches, num_patches))\n",
        "\n",
        "    for i in range(num_patches):\n",
        "        for j in range(num_patches):\n",
        "            if token_cols[i] < token_cols[j]:\n",
        "                left_mask[i, j] = 1\n",
        "            if token_cols[i] > token_cols[j]:\n",
        "                right_mask[i, j] = 1\n",
        "            if token_rows[i] < token_rows[j]:\n",
        "                up_mask[i, j] = 1\n",
        "            if token_rows[i] > token_rows[j]:\n",
        "                down_mask[i, j] = 1\n",
        "\n",
        "    identity_mask = np.eye(num_patches)\n",
        "    return left_mask, right_mask, up_mask, down_mask, identity_mask\n",
        "\n",
        "def create_mask_list_for_image(image, patch_size, num_heads, device, dtype):\n",
        "    \"\"\"\n",
        "    Compute a tuple of 5 torch masks, each expanded to (1, num_heads, N, N),\n",
        "    with an extra row/col for the CLS token.\n",
        "    \"\"\"\n",
        "    token_rows, token_cols = getRowsAndCols(image, patch_size)\n",
        "    left_np, right_np, up_np, down_np, identity_np = makeMasks(token_rows, token_cols)\n",
        "\n",
        "    left_np = np.pad(left_np, ((1,0),(1,0)), mode='constant', constant_values=1)\n",
        "    right_np = np.pad(right_np, ((1,0),(1,0)), mode='constant', constant_values=1)\n",
        "    up_np = np.pad(up_np, ((1,0),(1,0)), mode='constant', constant_values=1)\n",
        "    down_np = np.pad(down_np, ((1,0),(1,0)), mode='constant', constant_values=1)\n",
        "    identity_np = np.pad(identity_np, ((1,0),(1,0)), mode='constant', constant_values=1)\n",
        "\n",
        "    left_mask = torch.tensor(left_np, device=device, dtype=dtype).unsqueeze(0).unsqueeze(0)\n",
        "    right_mask = torch.tensor(right_np, device=device, dtype=dtype).unsqueeze(0).unsqueeze(0)\n",
        "    up_mask = torch.tensor(up_np, device=device, dtype=dtype).unsqueeze(0).unsqueeze(0)\n",
        "    down_mask = torch.tensor(down_np, device=device, dtype=dtype).unsqueeze(0).unsqueeze(0)\n",
        "    identity_mask = torch.tensor(identity_np, device=device, dtype=dtype).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    N = left_mask.shape[-1]\n",
        "    left_mask = left_mask.expand(1, num_heads, N, N)\n",
        "    right_mask = right_mask.expand(1, num_heads, N, N)\n",
        "    up_mask = up_mask.expand(1, num_heads, N, N)\n",
        "    down_mask = down_mask.expand(1, num_heads, N, N)\n",
        "    identity_mask = identity_mask.expand(1, num_heads, N, N)\n",
        "\n",
        "    return (left_mask, right_mask, up_mask, down_mask, identity_mask)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYcnuz5oI3TC"
      },
      "source": [
        "### This cell defines a dataset class that returns Food101 images, labels, and their corresponding spatial attention masks. It initializes the training and validation datasets and wraps them in data loaders. Finally, it prints the number of samples in each set. ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PN3bTXFrhv7T"
      },
      "outputs": [],
      "source": [
        "class OnTheFlyMaskedFood101(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, split, transform, patch_size, num_heads, download=True):\n",
        "        self.dataset = torchvision.datasets.Food101(\n",
        "            root=root, split=split, transform=transform, download=download\n",
        "        )\n",
        "        self.patch_size = patch_size\n",
        "        self.num_heads = num_heads\n",
        "        self.dtype = torch.float32\n",
        "        self.device = torch.device(\"cpu\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.dataset[idx]\n",
        "        mask_list = create_mask_list_for_image(\n",
        "            image, self.patch_size, self.num_heads, device=self.device, dtype=self.dtype\n",
        "        )\n",
        "        return image, label, mask_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "\n",
        "patch_size = 16 ####### Expiremental #######\n",
        "batch_size = 32 ####### Expiremental #######\n",
        "num_heads = 10 ####### Expiremental #######\n",
        "\n",
        "train_dataset = OnTheFlyMaskedFood101(\n",
        "    root='./data', split='train', transform=train_transform,\n",
        "    patch_size=patch_size, num_heads=num_heads, download=True\n",
        ")\n",
        "\n",
        "val_dataset = OnTheFlyMaskedFood101(\n",
        "    root='./data', split='test', transform=val_transform,\n",
        "    patch_size=patch_size, num_heads=num_heads, download=True\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Test/Val samples: {len(val_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzJ8o8MhJv4N"
      },
      "source": [
        "### This cell defines a custom attention module that uses five different key projections, each masked to focus on a specific spatial direction (left, right, up, down, identity). It initializes weights by copying from a standard ViT attention layer and computes five masked attention maps in the forward pass. The outputs from each attention head are summed and passed through a final projection layer. ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wSKxyanGhzft"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch.nn as nn\n",
        "\n",
        "class CustomAttentionMultipleFiveSpatial(nn.Module):\n",
        "    def __init__(self, orig_attn: nn.Module, patch_size=16, img_size=224):\n",
        "        super().__init__()\n",
        "        self.num_heads = orig_attn.num_heads\n",
        "        self.embed_dim = orig_attn.qkv.in_features\n",
        "        self.head_dim = self.embed_dim // self.num_heads\n",
        "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
        "        self.patch_size = patch_size\n",
        "        self.img_size = img_size\n",
        "\n",
        "\n",
        "        self.q_linear = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "        self.v_linear = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "        self.kA_linear = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "        self.kB_linear = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "        self.kC_linear = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "        self.kD_linear = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "        self.kE_linear = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "        self.proj = orig_attn.proj\n",
        "\n",
        "\n",
        "        qkv_weight = orig_attn.qkv.weight.clone()\n",
        "        qkv_bias = orig_attn.qkv.bias.clone() if orig_attn.qkv.bias is not None else None\n",
        "\n",
        "\n",
        "        self.q_linear.weight.data.copy_(qkv_weight[:self.embed_dim, :].clone())\n",
        "        self.kA_linear.weight.data.copy_(qkv_weight[self.embed_dim:2*self.embed_dim, :].clone())\n",
        "        self.kB_linear.weight.data.copy_(qkv_weight[self.embed_dim:2*self.embed_dim, :].clone())\n",
        "        self.kC_linear.weight.data.copy_(qkv_weight[self.embed_dim:2*self.embed_dim, :].clone())\n",
        "        self.kD_linear.weight.data.copy_(qkv_weight[self.embed_dim:2*self.embed_dim, :].clone())\n",
        "        self.kE_linear.weight.data.copy_(qkv_weight[self.embed_dim:2*self.embed_dim, :].clone())\n",
        "        self.v_linear.weight.data.copy_(qkv_weight[2*self.embed_dim:, :].clone())\n",
        "\n",
        "        if qkv_bias is not None:\n",
        "            self.q_linear.bias.data.copy_(qkv_bias[:self.embed_dim].clone())\n",
        "            self.kA_linear.bias.data.copy_(qkv_bias[self.embed_dim:2*self.embed_dim].clone())\n",
        "            self.kB_linear.bias.data.copy_(qkv_bias[self.embed_dim:2*self.embed_dim].clone())\n",
        "            self.kC_linear.bias.data.copy_(qkv_bias[self.embed_dim:2*self.embed_dim].clone())\n",
        "            self.kD_linear.bias.data.copy_(qkv_bias[self.embed_dim:2*self.embed_dim].clone())\n",
        "            self.kE_linear.bias.data.copy_(qkv_bias[self.embed_dim:2*self.embed_dim].clone())\n",
        "            self.v_linear.bias.data.copy_(qkv_bias[2*self.embed_dim:].clone())\n",
        "\n",
        "    def forward(self, query, key=None, value=None, key_padding_mask=None, need_weights=True, attn_mask=None):\n",
        "        if key is None:\n",
        "            key = query\n",
        "        if value is None:\n",
        "            value = query\n",
        "        if not hasattr(self, 'current_mask_list'):\n",
        "            raise ValueError(\"current_mask_list attribute not set in CustomAttentionMultipleFiveSpatial!\")\n",
        "        mask_list = self.current_mask_list\n",
        "        left_mask, right_mask, up_mask, down_mask, identity_mask = mask_list\n",
        "\n",
        "        if left_mask.shape[1] != self.num_heads:\n",
        "            left_mask = left_mask[:, :self.num_heads, :, :]\n",
        "            right_mask = right_mask[:, :self.num_heads, :, :]\n",
        "            up_mask = up_mask[:, :self.num_heads, :, :]\n",
        "            down_mask = down_mask[:, :self.num_heads, :, :]\n",
        "            identity_mask = identity_mask[:, :self.num_heads, :, :]\n",
        "\n",
        "        B, N, _ = query.shape\n",
        "        q = self.q_linear(query).reshape(B, N, self.num_heads, self.head_dim)\n",
        "        kA = self.kA_linear(query).reshape(B, N, self.num_heads, self.head_dim)\n",
        "        kB = self.kB_linear(query).reshape(B, N, self.num_heads, self.head_dim)\n",
        "        kC = self.kC_linear(query).reshape(B, N, self.num_heads, self.head_dim)\n",
        "        kD = self.kD_linear(query).reshape(B, N, self.num_heads, self.head_dim)\n",
        "        kE = self.kE_linear(query).reshape(B, N, self.num_heads, self.head_dim)\n",
        "        v = self.v_linear(query).reshape(B, N, self.num_heads, self.head_dim)\n",
        "\n",
        "        q = q.transpose(1, 2) * self.scale\n",
        "        kA = kA.transpose(1, 2)\n",
        "        kB = kB.transpose(1, 2)\n",
        "        kC = kC.transpose(1, 2)\n",
        "        kD = kD.transpose(1, 2)\n",
        "        kE = kE.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        attn_ka = (q @ kA.transpose(-2, -1)).softmax(dim=-1) * left_mask\n",
        "        attn_kb = (q @ kB.transpose(-2, -1)).softmax(dim=-1) * right_mask\n",
        "        attn_kc = (q @ kC.transpose(-2, -1)).softmax(dim=-1) * up_mask\n",
        "        attn_kd = (q @ kD.transpose(-2, -1)).softmax(dim=-1) * down_mask\n",
        "        attn_ke = (q @ kE.transpose(-2, -1)).softmax(dim=-1) * identity_mask\n",
        "\n",
        "        out_a = attn_ka @ v\n",
        "        out_b = attn_kb @ v\n",
        "        out_c = attn_kc @ v\n",
        "        out_d = attn_kd @ v\n",
        "        out_e = attn_ke @ v\n",
        "        out = out_a + out_b + out_c + out_d + out_e\n",
        "\n",
        "        out = out.transpose(1, 2).reshape(B, N, self.embed_dim)\n",
        "        out = self.proj(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkTD3ytHKARs"
      },
      "source": [
        "### This cell defines a custom Vision Transformer that replaces standard attention with the 5-direction masked attention in selected blocks. It keeps a configurable number of transformer blocks and injects CustomAttentionMultipleFiveSpatial into each. The model is instantiated, moved to GPU if available, and printed. ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5qqNzG1y9d2"
      },
      "outputs": [],
      "source": [
        "from timm.models.vision_transformer import vit_small_patch16_224\n",
        "import torch.nn as nn\n",
        "\n",
        "class ViTCustom(nn.Module):\n",
        "    def __init__(self, num_blocks_to_keep, patch_size=16, img_size=224):\n",
        "        super().__init__()\n",
        "\n",
        "        full_model = vit_small_patch16_224(pretrained=False, num_classes=101, drop_rate=0.3, drop_path_rate=0.1) ####### Expiremental #######\n",
        "\n",
        "        self.patch_embed = full_model.patch_embed\n",
        "        self.cls_token = full_model.cls_token\n",
        "        self.pos_embed = full_model.pos_embed\n",
        "        self.pos_drop = full_model.pos_drop\n",
        "\n",
        "        self.blocks = nn.Sequential()\n",
        "        for i, block in enumerate(full_model.blocks[:num_blocks_to_keep]):\n",
        "            if hasattr(block, 'attn'):\n",
        "                block.attn = CustomAttentionMultipleFiveSpatial(block.attn, patch_size=patch_size, img_size=img_size)\n",
        "                print(f\"Block {i}: Custom attention injected.\")\n",
        "            else:\n",
        "                raise AttributeError(f\"Block {i} has no attention module.\")\n",
        "            self.blocks.append(block)\n",
        "\n",
        "        self.norm = full_model.norm\n",
        "        self.head = full_model.head\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "        x = self.blocks(x)\n",
        "        x = self.norm(x)\n",
        "        return self.head(x[:, 0])\n",
        "\n",
        "model = ViTCustom(num_blocks_to_keep=10, patch_size=16, img_size=224) ####### Expiremental #######\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(device)\n",
        "print(model)\n",
        "print(\"Model created with custom attention and reduced layers.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8wjm3GgKes4"
      },
      "source": [
        "This cell defines a learning rate schedule using flat warmup, linear ramp-up, and cosine decay over 60 epochs. It calculates step-based progress for each phase and returns a scaling factor to apply to the base learning rate. This function will later be used with a PyTorch learning rate scheduler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cR8Lgdjwh5TR"
      },
      "outputs": [],
      "source": [
        "base_learning_rate = 1e-5 ####### Expiremental #######\n",
        "peak_learning_rate = 1e-4 ####### Expiremental #######\n",
        "final_lr_fraction = 0.10 ####### Expiremental #######\n",
        "\n",
        "num_epochs = 60 ####### Expiremental #######\n",
        "warmup_epochs = 15 ####### Expiremental #######\n",
        "rampup_epochs = 15 ####### Expiremental #######\n",
        "decay_epochs = num_epochs - (warmup_epochs + rampup_epochs)\n",
        "\n",
        "total_steps = len(train_loader) * num_epochs\n",
        "warmup_steps = len(train_loader) * warmup_epochs\n",
        "rampup_steps = len(train_loader) * rampup_epochs\n",
        "decay_steps = total_steps - warmup_steps - rampup_steps\n",
        "\n",
        "weight_decay = 1e-4 ####### Expiremental #######\n",
        "\n",
        "####### Expiremental #######\n",
        "def lr_lambda(step):\n",
        "    if step < warmup_steps:\n",
        "        return 1.0\n",
        "    elif step < warmup_steps + rampup_steps:\n",
        "        progress = (step - warmup_steps) / rampup_steps\n",
        "        scaled_lr = base_learning_rate + progress * (peak_learning_rate - base_learning_rate)\n",
        "        return scaled_lr / base_learning_rate\n",
        "    else:\n",
        "        progress = (step - warmup_steps - rampup_steps) / max(1, decay_steps)\n",
        "        cosine = 0.5 * (1 + math.cos(math.pi * progress))\n",
        "        scaled_lr = final_lr_fraction * peak_learning_rate + (1 - final_lr_fraction) * peak_learning_rate * cosine\n",
        "        return scaled_lr / base_learning_rate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_Vri4F5KncD"
      },
      "source": [
        "### This cell sets up the training components: cross-entropy loss with label smoothing, the AdamW optimizer, and a LambdaLR scheduler that applies the custom learning rate schedule. ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OOwKqu1fYI0m"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=base_learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr0z82A8KvTY"
      },
      "source": [
        "### This cell runs the training and validation loop, applying spatial masks to each attention block during forward passes. It logs metrics to TensorBoard, updates the learning rate scheduler per step, and tracks the best model weights based on validation accuracy. The best model is restored at the end. ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrLLO5Y4h8sv"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "writer = SummaryWriter('runs/food101_multiK_experiment')\n",
        "best_val_acc = 0.0\n",
        "best_model_wts = None\n",
        "\n",
        "global_step = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_start = time.time()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels, batch_mask_list in tqdm(train_loader, desc=\"Training\", leave=False):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        batch_mask_list = tuple(m.squeeze(1).to(device) for m in batch_mask_list)\n",
        "        \n",
        "        for i in range(len(model.blocks)):\n",
        "            model.blocks[i].attn.current_mask_list = batch_mask_list\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        scheduler.step()\n",
        "        global_step += 1\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    epoch_acc = running_corrects.double() / len(train_dataset)\n",
        "    print(f\"Train Loss: {epoch_loss:.4f}  Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    val_running_corrects = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels, batch_mask_list in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            batch_mask_list = tuple(m.squeeze(1).to(device) for m in batch_mask_list)\n",
        "\n",
        "            for i in range(len(model.blocks)):\n",
        "                model.blocks[i].attn.current_mask_list = batch_mask_list\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_running_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    val_epoch_loss = val_running_loss / len(val_dataset)\n",
        "    val_epoch_acc = val_running_corrects.double() / len(val_dataset)\n",
        "    print(f\"Val Loss: {val_epoch_loss:.4f}  Val Acc: {val_epoch_acc:.4f}\")\n",
        "\n",
        "    if val_epoch_acc > best_val_acc:\n",
        "        best_val_acc = val_epoch_acc\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    epoch_duration = time.time() - epoch_start\n",
        "    print(f\"Epoch {epoch+1} completed in {epoch_duration:.2f} seconds\\n\")\n",
        "\n",
        "if best_model_wts is not None:\n",
        "    model.load_state_dict(best_model_wts)\n",
        "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
        "\n",
        "writer.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
