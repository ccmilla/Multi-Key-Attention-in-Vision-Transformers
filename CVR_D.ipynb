{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training vit_small_patch16_224 from scratch (no pre-trained weights) on the Food-101 dataset with the default attention block #\n",
        "## Author - Thomas O'Sullivan ##\n",
        "\n",
        "### This notebook reduces the model to have 10/12 attention blocks, a drop rate of 0.3, batch size of 32, weight decay of 1e-4, and is trained with a base learning rate of 1e-5 as it progresses through our LR schedule defined in cell 4. ###"
      ],
      "metadata": {
        "id": "dNSC7WMSl66o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This cell imports libraries for deep learning, data handling, and visualization, and sets random seeds for reproducibility. ###"
      ],
      "metadata": {
        "id": "Qj51L_X_DGtl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5pS-u3nUCVut"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import copy\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "torch.manual_seed(31)\n",
        "random.seed(31)\n",
        "np.random.seed(31)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This cell defines image transformations for training and validation. Training data is augmented with cropping, flipping, and color jittering, while validation data is resized and center cropped. Both are converted to tensors and normalized using ImageNet statistics. ###"
      ],
      "metadata": {
        "id": "w-nO-INhDbIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "metadata": {
        "id": "PQFvQXbhl1JC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This cell loads the Food101 dataset with the defined transforms and creates data loaders for training and validation. It sets a batch size of 32 and enables shuffling for the training loader. It also prints the number of samples in each set. ###"
      ],
      "metadata": {
        "id": "mCO5DXlrDyQJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Xd8mjnaLC0FY"
      },
      "outputs": [],
      "source": [
        "train_dataset = torchvision.datasets.Food101(\n",
        "    root='./data',\n",
        "    split='train',\n",
        "    transform=train_transform,\n",
        "    download=True\n",
        ")\n",
        "\n",
        "val_dataset = torchvision.datasets.Food101(\n",
        "    root='./data',\n",
        "    split='test',\n",
        "    transform=val_transform,\n",
        "    download=True\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)  ####### Expiremental #######\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)  ####### Expiremental #######\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Val samples:   {len(val_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This cell defines a custom Vision Transformer model using only the first 10 transformer blocks of vit_small_patch16_224. It disables pretraining, sets dropout rates, and customizes the forward pass. The model is then moved to GPU if available and printed. ###"
      ],
      "metadata": {
        "id": "MRJxLopAD7py"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "20zWTzxq6hCB"
      },
      "outputs": [],
      "source": [
        "from timm import create_model\n",
        "import torch.nn as nn\n",
        "\n",
        "class ViTLayerReduction(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        full_model = create_model(\n",
        "            \"vit_small_patch16_224\",\n",
        "            pretrained=False,\n",
        "            num_classes=101,  ####### Expiremental #######\n",
        "            drop_rate=0.3,  ####### Expiremental #######\n",
        "            drop_path_rate=0.1 ####### Expiremental #######\n",
        "        )\n",
        "\n",
        "        self.patch_embed = full_model.patch_embed\n",
        "        self.cls_token = full_model.cls_token\n",
        "        self.pos_embed = full_model.pos_embed\n",
        "        self.pos_drop = full_model.pos_drop\n",
        "\n",
        "        self.blocks = nn.Sequential(*list(full_model.blocks[:10]))  ####### Expiremental #######\n",
        "\n",
        "        self.norm = full_model.norm\n",
        "        self.head = full_model.head\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "        x = self.blocks(x)\n",
        "        x = self.norm(x)\n",
        "        return self.head(x[:, 0])\n",
        "\n",
        "\n",
        "model = ViTLayerReduction()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(device)\n",
        "print(model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This cell defines the learning rate schedule using warmup, linear ramp-up, and cosine decay phases. It also sets hyperparameters like learning rates, epoch counts, and weight decay. The compute_scheduled_lr function calculates the learning rate for a given epoch. ###"
      ],
      "metadata": {
        "id": "svWGcPsaEUX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_learning_rate = 1e-5 ####### Expiremental #######\n",
        "peak_learning_rate = 1e-4 ####### Expiremental #######\n",
        "final_lr_fraction = 0.10 ####### Expiremental #######\n",
        "\n",
        "num_epochs = 60 ####### Expiremental #######\n",
        "warmup_epochs = 15 ####### Expiremental #######\n",
        "rampup_epochs = 15 ####### Expiremental #######\n",
        "decay_epochs = num_epochs - (warmup_epochs + rampup_epochs)\n",
        "\n",
        "weight_decay = 1e-4 ####### Expiremental #######\n",
        "\n",
        "lr_history = []\n",
        "\n",
        "####### Expiremental #######\n",
        "def compute_scheduled_lr(epoch_step):\n",
        "    \"\"\"Flat Warmup -> Linear Rampup -> Cosine Decay.\"\"\"\n",
        "    if epoch_step < warmup_epochs:\n",
        "        return base_learning_rate\n",
        "    elif epoch_step < warmup_epochs + rampup_epochs:\n",
        "        progress = (epoch_step - warmup_epochs) / rampup_epochs\n",
        "        return base_learning_rate + progress * (peak_learning_rate - base_learning_rate)\n",
        "    else:\n",
        "        decay_progress = (epoch_step - warmup_epochs - rampup_epochs) / max(1, decay_epochs)\n",
        "        cosine_decay = 0.5 * (1 + math.cos(math.pi * decay_progress))\n",
        "        return final_lr_fraction * peak_learning_rate + (1 - final_lr_fraction) * peak_learning_rate * cosine_decay\n",
        "\n"
      ],
      "metadata": {
        "id": "IEXtCVjA9ryi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This cell trains the model over multiple epochs using the AdamW optimizer and label smoothed cross entropy loss. It logs training/validation loss, accuracy, and learning rate to TensorBoard, while applying the custom learning rate schedule. The model weights with the best validation accuracy are saved and restored at the end. ###"
      ],
      "metadata": {
        "id": "X9CLjAu2E0XP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5r5_rHP6ETnF"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.AdamW(model.parameters(), lr=base_learning_rate, weight_decay=weight_decay)\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "writer = SummaryWriter('runs/food101_vit_experiment')\n",
        "\n",
        "best_val_acc = 0.0\n",
        "best_model_wts = None\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_start = time.time()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=\"Training\", leave=False):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    epoch_acc = running_corrects.double() / len(train_dataset)\n",
        "    print(f\"Train Loss: {epoch_loss:.4f}  Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    val_running_corrects = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_running_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    val_epoch_loss = val_running_loss / len(val_dataset)\n",
        "    val_epoch_acc = val_running_corrects.double() / len(val_dataset)\n",
        "    print(f\"Val Loss: {val_epoch_loss:.4f}  Val Acc: {val_epoch_acc:.4f}\")\n",
        "\n",
        "\n",
        "    writer.add_scalar('Loss/Train', epoch_loss, epoch)\n",
        "    writer.add_scalar('Accuracy/Train', epoch_acc.item(), epoch)\n",
        "    writer.add_scalar('Loss/Validation', val_epoch_loss, epoch)\n",
        "    writer.add_scalar('Accuracy/Validation', val_epoch_acc.item(), epoch)\n",
        "\n",
        "    current_lr_epoch = epoch\n",
        "    new_lr = compute_scheduled_lr(current_lr_epoch)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = new_lr\n",
        "    lr_history.append(new_lr)\n",
        "    writer.add_scalar('Learning Rate', new_lr, epoch)\n",
        "\n",
        "    if val_epoch_acc > best_val_acc:\n",
        "        best_val_acc = val_epoch_acc\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    epoch_duration = time.time() - epoch_start\n",
        "    print(f\"Epoch {epoch+1} completed in {epoch_duration:.2f} seconds\\n\")\n",
        "\n",
        "if best_model_wts is not None:\n",
        "    model.load_state_dict(best_model_wts)\n",
        "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
        "\n",
        "writer.close()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
