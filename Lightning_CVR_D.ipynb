{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using PyTorch lightning from Multi-Key-Attention-in-Vision-Transformers which is just using PyTorch.  \n",
    "\n",
    "Multi-Key-Attention-in-Vision-Transformer uses training vit_small_patch16_224 from scratch (no pre-trained weights) on the Food-101 dataset with the default attention block\n",
    "\n",
    "## the original notebook reduces the model to have:\n",
    "10/12 attention block\n",
    "a drop rate of .3\n",
    "drop path rate of .1\n",
    "batch size of 32\n",
    "weight decay of 1e-4\n",
    "is trained with a base learning rate of 1e-5\n",
    "as it progresses through LR schedule defined in cell 4.\n",
    "\n",
    "## change as of 10/2025 ##\n",
    "\n",
    "drop rate of .5\n",
    "drop path rate of .2\n",
    "weight decay is .05\n",
    "\n",
    "## This cell imports libraries for deep learning, data handling, visualization, and sets random seeds for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as pyplot\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#Seeding random\n",
    "torch.manual_seed(31)\n",
    "random.seed(31)\n",
    "np.random.seed(31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This cell defines image transformations for training and validation.  Training data is augmented with cropping, flipping, and color jittering, while validation data is resized and center cropped.  Both are converted to tensors and normalized using ImageNet statistics. ##\n",
    "\n",
    "train_transform = transforms.Compose([transforms.RandomSizedCrop(224, scale(0.8, 1.0)), \n",
    "                                      transforms.RandomHorizontalFlip(), \n",
    "                                      transform.Colorjitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "val_transform = transforms.Compose([transforms.Resize(256),\n",
    "                                    transforms.CenterCrop(224),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    tranforms.Normalize(mean[0.485, 0.456, 0.406])])\n",
    "            \n",
    "## Taking the above code and putting it into the ImageDataModule .setup() creates datasets using transforms train_dataloader() builds the training DataLoader val_dataloader() builds the validation Dataloader - this is auto generated in the init so it's cleaner. ##\n",
    "Organize Datasets into a lightningDataModule in PyTorch Lightning, it's best practice to use a LightningDataModule to handle your dataset loading and transforms.  Wrapping transforms and dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class ImageDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir, batch_size=32, num_workers=4): #what's data dir?\n",
    "        super().__init__\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.train_transform = transforms.Compose(\n",
    "            transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229,0.224,0.225])\n",
    "        )\n",
    "        self.val_transform = transforms.Compose(\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "        )\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = datasets.ImageFolder(\n",
    "            root=self.data_dir,\n",
    "            split='train',\n",
    "            transform=self.train_transform,\n",
    "            download=True\n",
    "        )\n",
    "        self.val_dataset = datasets.ImageFolder(\n",
    "            root=self.data_dir,\n",
    "            split='train',\n",
    "            transform=self.val_transform,\n",
    "            download=True\n",
    "        )\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding this later to Trainer: data_module = ImageDataModule(data_dir='path/to/dataset', batch_size=32) model = YourLightningModel() # changing this once I get the model trainer = pl.Trainer(max_epochs=10) trainer.fit(model, datamodule=data_module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
